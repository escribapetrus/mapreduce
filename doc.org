* A Simple Erlang Implementation of MapReduce

MapReduce is a massively parallel program for data analysis. Its purpose is to abstract
the implementation details of parallel computation, leaving the programmer with the simple task
of defining two functions that describe how the data is supposed to be computed.

MapReduce is also a classic example in the discipline of concurrency and distributed systems.
I first learned about the program as the introduction to the MIT 6824 Distributed Systems course,
available on YouTube. So, as a form of exercise in this field, I wanted to implement it
in the language I am currently studying, Erlang.

** Erlang -- why and what
(If you are familiar with Erlang, the BEAM, processes, and supervisors, feel free to skip this section.)

Erlang was developed as a language for distributed systems, with support for concurrency
and fault tolerance at its core. Joe Armstrong, its creator, defined its style as
"concurrency-oriented programming". Though the formula "something-oriented programming" has
basically become a meme at this point, it is easy to understand why the creator would use
such an expression to define the language.
 
Erlang provides concurrency and fault tolerance by means of processes, supervisors
and a Virtual Machine that handles the sheduling of computation by the processor.
Let us briefly see what those are.

*** Processes
Processes are basic units of concurrent execution.

Processes can be spawned as simple lambda functions that execute some instructions and exit
(in fact, we do just that in our MapReduce program, where keys are processed by spawning processes).
Most commonly, however, they are spawned as tail-recursive functions -- loops that can store state,
receive messages and work as lightweight servers.

As pure functions, processes are memory-isolated, and do not take global variables or shared state.
This provides a safeguard against race-conditions, and avoids the need to implement constructs
such as mutexes -- you don't need to lock memory if memory is not shared at all.

*** Supervisors
Supervisors are processes that start, monitor and restart failed processes, providing fault-tolerance.

They are part of the OTP (Open Telecom Platform) pattern of designing systems with Erlang.
Applications are structured as supervision trees, where parent processes spawn children processes,
which can themselves be supervisors, servers, state machines and other types of processes.

*** Erlang Virtual Machine (BEAM)
Like other virtual machines in the computer space, the Erlang VM (BEAM) is a program that sits on top
of the operating system and runs code generated by the Erlang compiler. In that sense, it is not
very different from other systems like the JVM.

What is special is that it provides an efficient built-in support for handling the concurrent execution
of processes, transparently scheduling their turns in the processor.

*** Functional programming
Erlang is a functional programming language, a paradigm defined by immutable data and pure functions.
...

It is not, howerver, the purest functional programming language. Erlang allows calling side-efects
such io and database queries in the middle of the function body, and it would be hard to provide an
equational reasoning for the concurrency model of communication between processes.


** MapReduce
MapReduce was created by software engineers at Google when they realised that many data analysis
programs were structured in the same pattern: mapping data to key-value pairs and reducing the
values in every key to a single result.

In order to encapsulate the distributed architecture in a library, the engineers designed the program
as a higher-order function, inspired by functional programming patterns in languages like Lisp.

A higher-order function is a function that takes functions as arguments
(a function as a data type, also called a lambda),and computes the result
of applying these to other arguments. In the case of MapReduce,
the program takes a Map function, a Reduce function, and a list of inputs (keys, files) to compute.
It then applies Map and Reduce to every item in the input, concurrently.

*** Map
Map is the function that describes how to process individual data items, returning a key value pair.
This key will be used for grouping data items to be processed by Reduce.

The Map function has the following signature, in Erlang syntax:

#+BEGIN_SRC erlang
    ({K1 :: binary(), V1 :: binary()}) ->
      {K2 :: binary(), V2 :: binary()} | list({K2, V2})
#+END_SRC

Input values (V1) can be parsed from binary (for example, a JSON string) into Erlang terms like maps,
in order to produce the output values we need. For example:

#+BEGIN_SRC erlang


#+END_SRC



%% Mapper is a gen_server that processes a list of keys,
%% fetching files by keys (filenames) in the file system (fs) 
%% and applying the Map function.
%%
%% The Map function has the following signature:
%% ({K :: binary(), V :: binary()}) -> {Key :: binary(), V :: binary()}
%%
%% every key spawns a process concurrently maps the data.

    


*** File Systems
The key-value pairs computed by the Map and Reduce functions represent files in the MapReduce file system.
In our implementation, these are simple folders in the project, but in the real world it would simply be a
network file system (NFS) whose implementation is transparent to MapReduce.
    
In my first implementation of MapReduce, I didn't use files. This meant that all the data that was being processed
had to be stored in the gen_server state -- that is, in memory.

The file system simplified the architecture so much. Now, instead of storing all data in the process state, the data
can be simply read from the file system, processed, saved to new files.

Testing and handling errors is also much more simple. We can simply call values that are not available in the file system,
ask the program to process these filenames, and watch it crash and recover.
    
*** Abstracting the distributed architecture
Running a program in a distributed system requires much more work than running it in a sequential,
single thread, single node program. The setup requires things like:

- provisioning worker nodes;
- provisioning replicas for fault tolerance;
- provisioning a shared file system;
- setting up a network;
- defining an API;
- defining rules for replica takeover.

  None of these things are essentially related to the task in a logical sense.
  But they are essential for running the program...
